{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mortgage Lending Fair Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using conda environment \"fairfs\" (Python 3.8)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector\n",
    "from sklearn import metrics, model_selection, pipeline, preprocessing\n",
    "from sklearn import linear_model, naive_bayes, tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#import dataset_loader\n",
    "#import fairfs\n",
    "import unfairness_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROTECTED_COLUMN = 'derived_race'  # 'group' for simulated data, 'sex_Female' for adult, 'rural' for other datasets\n",
    "ITERATIONS = 100\n",
    "ACCURACY_METRIC = metrics.roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('hmda_mortgage_derived_race_ffs.csv', header=0)\n",
    "X = df.drop('action_taken', axis=1)\n",
    "y = df['action_taken']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "income                               float64\n",
       "debt_to_income_ratio                 float64\n",
       "combined_loan_to_value_ratio         float64\n",
       "loan_amount                            int64\n",
       "derived_ethnicity                      int64\n",
       "derived_race                           int64\n",
       "derived_sex                            int64\n",
       "applicant_age                          int64\n",
       "preapproval                            int64\n",
       "loan_type                              int64\n",
       "loan_purpose                           int64\n",
       "interest_only_payment                  int64\n",
       "balloon_payment                        int64\n",
       "loan_term                              int64\n",
       "property_value                       float64\n",
       "state_code                             int64\n",
       "county_code                          float64\n",
       "tract_minority_population_percent    float64\n",
       "tract_population                       int64\n",
       "tract_to_msa_income_percentage       float64\n",
       "tract_median_age_of_housing_units      int64\n",
       "ffiec_msa_md_median_family_income      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mortgage_data():\n",
    "    # Simple simulated classification dataset with unfair and fair features\n",
    "    df = pd.read_csv('hmda_mortgage_derived_race_ffs.csv', header=0)\n",
    "    X = df.drop('action_taken', axis=1)\n",
    "    y = df['action_taken']\n",
    "    return {\n",
    "        'mortgage_data': {\n",
    "            'data': X.values,\n",
    "            'labels': y.values,\n",
    "            'participant_ids': np.arange(0, len(df)),\n",
    "            'feature_names': np.array([f for f in df if f not in ['action_taken']])\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(X, y, clf, protected_groups, unfairness_metric, unfairness_weight):\n",
    "    metric = unfairness_metrics.UnfairnessMetric(protected_groups, unfairness_metric)\n",
    "    unfairness_scorer = metrics.make_scorer(metric)\n",
    "    unfairness_means = []\n",
    "    auc_means = []\n",
    "    selected_feature_props = np.zeros([ITERATIONS, X.shape[1]])\n",
    "    for i in tqdm(range(ITERATIONS), desc=' Training ' + clf.__class__.__name__):\n",
    "        xval = model_selection.KFold(4, shuffle=True, random_state=i)\n",
    "        # Make a metric combining accuracy and subtracting unfairness w.r.t. the protected groups\n",
    "        metric = unfairness_metrics.CombinedMetric(ACCURACY_METRIC, protected_groups,\n",
    "                                                   unfairness_metric, unfairness_weight)\n",
    "        combined_scorer = metrics.make_scorer(metric)\n",
    "        sfs = SequentialFeatureSelector(clf, 'best', verbose=0, cv=xval, scoring=combined_scorer,\n",
    "                                        n_jobs=2)\n",
    "        pipe = pipeline.Pipeline([\n",
    "            ('standardize', preprocessing.StandardScaler()),\n",
    "            ('feature_selection', sfs),\n",
    "            ('model', clf),\n",
    "        ])\n",
    "        result = model_selection.cross_validate(pipe, X, y, verbose=0, cv=xval, scoring={\n",
    "            'unfairness': unfairness_scorer,\n",
    "            'auc': metrics.make_scorer(ACCURACY_METRIC),\n",
    "        }, return_estimator=True)\n",
    "        unfairness_means.append(result['test_unfairness'].mean())\n",
    "        auc_means.append(result['test_auc'].mean())\n",
    "        for estimator in result['estimator']:\n",
    "            for feature_i in estimator.named_steps['feature_selection'].k_feature_idx_:\n",
    "                selected_feature_props[i][feature_i] += 1 / len(result['estimator'])\n",
    "    return unfairness_means, auc_means, selected_feature_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'labels', 'participant_ids', 'feature_names'])\n"
     ]
    }
   ],
   "source": [
    "ds = get_mortgage_data()['mortgage_data']\n",
    "print(ds.keys())  # data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[1.6500e+02, 1.0000e+00, 6.3000e+01, ..., 9.0000e+01, 3.4000e+01,\n",
       "         4.9800e+04],\n",
       "        [1.1700e+02, 0.0000e+00, 7.9000e+01, ..., 1.0700e+02, 2.9000e+01,\n",
       "         5.3400e+04],\n",
       "        [9.3000e+01, 0.0000e+00, 8.7000e+01, ..., 7.4000e+01, 3.8000e+01,\n",
       "         5.6100e+04],\n",
       "        ...,\n",
       "        [6.2000e+01, 4.0000e+00, 5.8214e+01, ..., 8.5000e+01, 3.8000e+01,\n",
       "         7.2900e+04],\n",
       "        [5.3000e+01, 3.0000e+00, 8.0000e+01, ..., 9.1000e+01, 6.2000e+01,\n",
       "         6.6900e+04],\n",
       "        [8.7000e+01, 4.0000e+00, 3.5556e+01, ..., 1.0600e+02, 4.9000e+01,\n",
       "         7.5300e+04]]),\n",
       " 'labels': array([1, 0, 1, ..., 1, 1, 0], dtype=int64),\n",
       " 'participant_ids': array([    0,     1,     2, ..., 73944, 73945, 73946]),\n",
       " 'feature_names': array(['income', 'debt_to_income_ratio', 'combined_loan_to_value_ratio',\n",
       "        'loan_amount', 'derived_ethnicity', 'derived_race', 'derived_sex',\n",
       "        'applicant_age', 'preapproval', 'loan_type', 'loan_purpose',\n",
       "        'interest_only_payment', 'balloon_payment', 'loan_term',\n",
       "        'property_value', 'state_code', 'county_code',\n",
       "        'tract_minority_population_percent', 'tract_population',\n",
       "        'tract_to_msa_income_percentage',\n",
       "        'tract_median_age_of_housing_units',\n",
       "        'ffiec_msa_md_median_family_income'], dtype='<U33')}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a column to use as the \"protected\" group labels\n",
    "protected_col_index = np.nonzero(ds['feature_names'] == PROTECTED_COLUMN)[0][0]\n",
    "protected_groups = pd.Series(ds['data'][:, protected_col_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1.0\n",
       "1        1.0\n",
       "2        1.0\n",
       "3        1.0\n",
       "4        1.0\n",
       "        ... \n",
       "73942    1.0\n",
       "73943    1.0\n",
       "73944    1.0\n",
       "73945    1.0\n",
       "73946    1.0\n",
       "Length: 73947, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protected_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does the method reduce unfairness?\n",
    "dfs = []\n",
    "try:\n",
    "    dfs.append(pd.read_csv('fairfs_results.csv'))\n",
    "except FileNotFoundError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DecisionTreeClassifier\n",
      "Unfairness metric: statistical_parity\n",
      "Unfairness metric weight: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Training DecisionTreeClassifier:   3%|█▎                                          | 3/100 [19:21<10:27:28, 388.13s/it]"
     ]
    }
   ],
   "source": [
    " for m in [tree.DecisionTreeClassifier(max_depth=3,random_state=11798), naive_bayes.GaussianNB(), linear_model.LogisticRegression(random_state=11798)]:\n",
    "    for unfairness_metric in unfairness_metrics.UNFAIRNESS_METRICS:\n",
    "        for unfairness_weight in [0, 1, 2, 3, 4]:\n",
    "            print('Training', m.__class__.__name__)\n",
    "            print('Unfairness metric:', unfairness_metric)\n",
    "            print('Unfairness metric weight:', unfairness_weight)\n",
    "            if len(dfs) > 0 and sum((dfs[0].model == m.__class__.__name__) &\n",
    "                                    (dfs[0].unfairness_metric == unfairness_metric) &\n",
    "                                    (dfs[0].unfairness_weight == unfairness_weight)) > 0:\n",
    "                print('Skipping (already done in output file)')\n",
    "                continue\n",
    "            unfairnesses, aucs, feature_selected_props = run_experiment(\n",
    "                ds['data'], pd.Series(ds['labels']), m, protected_groups, unfairness_metric,\n",
    "                unfairness_weight)\n",
    "            dfs.append(pd.DataFrame({\n",
    "                'model': [m.__class__.__name__] * len(aucs),\n",
    "                'unfairness_metric': [unfairness_metric] * len(aucs),\n",
    "                'unfairness_weight': [unfairness_weight] * len(aucs),\n",
    "                'iteration': range(1, len(aucs) + 1),\n",
    "                'unfairness': unfairnesses,\n",
    "                'auc': aucs,\n",
    "                'protected_column_selected_prop': feature_selected_props[:, protected_col_index],\n",
    "            }))\n",
    "            # What features does the model favor if it is optimizing for unfairness?\n",
    "            if 'fair_feature' in ds['feature_names']:  # Synthetic data\n",
    "                for col in ['fair_feature', 'unfair_feature']:\n",
    "                    col_index = np.nonzero(ds['feature_names'] == col)[0][0]\n",
    "                    dfs[-1][col + '_selected_prop'] = feature_selected_props[:, col_index]\n",
    "            pd.concat(dfs).to_csv('fairfs_results_sp.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
